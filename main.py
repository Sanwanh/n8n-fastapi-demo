#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å¸‚å ´åˆ†æå ±å‘Šç³»çµ± - APIæœå‹™ (ä¿®æ­£ç‰ˆ)
ä¸»è¦ä¿®æ­£ï¼š
1. ä¿®æ­£å¸‚å ´æ•¸æ“šé¡¯ç¤ºå•é¡Œ
2. å¢å¼·éŒ¯èª¤è™•ç†å’Œæ—¥èªŒ
3. ç¢ºä¿æ•¸æ“šæ­£ç¢ºå‚³éåˆ°å‰ç«¯
"""

import os
import sys
import json
import logging
from datetime import datetime, timedelta
from pathlib import Path
from typing import Dict, Optional, Any
import asyncio

# ç¬¬ä¸‰æ–¹å¥—ä»¶
try:
    from fastapi import FastAPI, Request, HTTPException
    from fastapi.responses import JSONResponse, HTMLResponse
    from fastapi.staticfiles import StaticFiles
    from fastapi.middleware.cors import CORSMiddleware
    from pydantic import BaseModel, EmailStr, field_validator
    import uvicorn
    import requests
    import yfinance as yf
    import pandas as pd
    import numpy as np
except ImportError as e:
    print(f"âŒ ç¼ºå°‘å¿…è¦çš„å¥—ä»¶: {e}")
    print("è«‹åŸ·è¡Œ: pip install -r requirements.txt")
    sys.exit(1)

# è¨­å®šæ—¥èªŒ - å¢å¼·ç‰ˆæœ¬
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(sys.stdout),
        logging.FileHandler('logs/market_analysis.log', encoding='utf-8')
    ]
)
logger = logging.getLogger(__name__)

# ç¢ºä¿ç›®éŒ„å­˜åœ¨
Path("logs").mkdir(exist_ok=True)
Path("data").mkdir(exist_ok=True)
Path("frontend/static").mkdir(parents=True, exist_ok=True)


# è¼‰å…¥é…ç½®
def load_config():
    return {
        'SERVER_CONFIG': {
            'host': os.getenv('SERVER_HOST', '0.0.0.0'),
            'port': int(os.getenv('SERVER_PORT', 8089)),
            'debug': os.getenv('DEBUG', 'False').lower() == 'true'
        },
        'WEBHOOK_CONFIG': {
            'send_url': os.getenv(
                'WEBHOOK_URL',
                'https://beloved-swine-sensibly.ngrok-free.app/webhook-test/ef5ac185-f41a-4a2d-9a78-33d329184c2'
            ),
            'n8n_webhook_url': 'https://beloved-swine-sensibly.ngrok-free.app/webhook/Webhook - Preview',
            'timeout': int(os.getenv('WEBHOOK_TIMEOUT', 30))
        },
        'SYSTEM_INFO': {
            'name': 'Market Analysis API',
            'version': '2.1.4',
            'description': 'æ™ºèƒ½å¸‚å ´åˆ†æAPIæœå‹™'
        }
    }


CONFIG = load_config()


# è³‡æ–™æ¨¡å‹
class N8NDataExtended(BaseModel):
    positive: int
    neutral: int
    negative: int
    summary: str
    score: int
    label: str
    emailReportHtml: str

    @field_validator('average_sentiment_score')
    @classmethod
    def validate_score(cls, v):
        if not 0 <= v <= 100:
            raise ValueError('æƒ…æ„Ÿåˆ†æ•¸å¿…é ˆåœ¨ 0 åˆ° 100 ä¹‹é–“')
        return v


class MailSenderRequest(BaseModel):
    recipient_email: EmailStr
    sender_name: Optional[str] = "å¸‚å ´åˆ†æç³»çµ±"
    subject: Optional[str] = "å¸‚å ´åˆ†æå ±å‘Š"
    priority: Optional[str] = "normal"
    mail_type: Optional[str] = "daily"
    custom_message: Optional[str] = ""
    include_charts: bool = False
    include_recommendations: bool = False
    include_risk_warning: bool = False


# ç”Ÿå‘½é€±æœŸç®¡ç†
from contextlib import asynccontextmanager


@asynccontextmanager
async def lifespan(app: FastAPI):
    """æ‡‰ç”¨ç”Ÿå‘½é€±æœŸç®¡ç†"""
    # å•Ÿå‹•æ™‚
    logger.info("ğŸš€ å¸‚å ´åˆ†æç³»çµ±å•Ÿå‹• - ä¿®æ­£ç‰ˆ")
    logger.info(f"ğŸ“¡ N8N Webhook: {CONFIG['WEBHOOK_CONFIG']['n8n_webhook_url']}")
    logger.info(f"ğŸŒ ä¸»ç¶²ç«™: http://{CONFIG['SERVER_CONFIG']['host']}:{CONFIG['SERVER_CONFIG']['port']}")
    logger.info(f"ğŸ“§ éƒµä»¶é é¢: http://{CONFIG['SERVER_CONFIG']['host']}:{CONFIG['SERVER_CONFIG']['port']}/mail")
    logger.info(f"ğŸ“– APIæ–‡æª”: http://{CONFIG['SERVER_CONFIG']['host']}:{CONFIG['SERVER_CONFIG']['port']}/api/docs")

    # æ¸¬è©¦é»ƒé‡‘åƒ¹æ ¼ API
    try:
        logger.info("ğŸ” æ¸¬è©¦é»ƒé‡‘åƒ¹æ ¼ API...")
        import yfinance as yf
        test_ticker = yf.Ticker("GC=F")
        test_data = test_ticker.history(period="1d")
        if not test_data.empty:
            logger.info("âœ… é»ƒé‡‘åƒ¹æ ¼ API é€£æ¥æ­£å¸¸")
        else:
            logger.warning("âš ï¸ é»ƒé‡‘åƒ¹æ ¼ API å¯èƒ½æœ‰å•é¡Œï¼Œå°‡ä½¿ç”¨æ¨¡æ“¬æ•¸æ“š")
    except Exception as e:
        logger.warning(f"âš ï¸ é»ƒé‡‘åƒ¹æ ¼ API æ¸¬è©¦å¤±æ•—: {str(e)}ï¼Œå°‡ä½¿ç”¨æ¨¡æ“¬æ•¸æ“š")

    yield

    # é—œé–‰æ™‚
    logger.info("ğŸ›‘ å¸‚å ´åˆ†æç³»çµ±é—œé–‰ä¸­...")


# åˆå§‹åŒ– FastAPI
app = FastAPI(
    title=CONFIG['SYSTEM_INFO']['name'],
    description=CONFIG['SYSTEM_INFO']['description'],
    version=CONFIG['SYSTEM_INFO']['version'],
    docs_url="/api/docs",
    redoc_url="/api/redoc",
    lifespan=lifespan
)

# CORS è¨­å®š
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# æ›è¼‰éœæ…‹æª”æ¡ˆ
app.mount("/static", StaticFiles(directory="frontend/static"), name="static")


# Web è·¯ç”±
@app.get("/", response_class=HTMLResponse)
async def home():
    """ä¸»é é¢ - é¡¯ç¤ºå¸‚å ´æ•¸æ“šå’Œé»ƒé‡‘åƒ¹æ ¼"""
    html_file = Path("frontend") / "index.html"
    if html_file.exists():
        with open(html_file, 'r', encoding='utf-8') as f:
            return HTMLResponse(content=f.read())
    return HTMLResponse(content="<h1>é¦–é æª”æ¡ˆä¸å­˜åœ¨</h1>", status_code=404)


@app.get("/mail", response_class=HTMLResponse)
async def mail_page():
    """éƒµä»¶ç™¼é€é é¢"""
    html_file = Path("frontend") / "mail.html"
    if html_file.exists():
        with open(html_file, 'r', encoding='utf-8') as f:
            return HTMLResponse(content=f.read())
    return HTMLResponse(content="<h1>éƒµä»¶é é¢æª”æ¡ˆä¸å­˜åœ¨</h1>", status_code=404)


# å…¨åŸŸè®Šæ•¸ - å¢å¼·ç‰ˆæœ¬
stored_data = {}
system_stats = {
    "total_reports": 0,
    "today_reports": 0,
    "last_reset": datetime.now().date(),
    "uptime_start": datetime.now(),
    "last_data_received": None,
    "api_calls": 0,
    "gold_price_calls": 0,
    "errors": 0
}


# API è·¯ç”±
@app.post("/api/n8n-data")
async def receive_n8n_data(request: Request):
    """æ¥æ”¶ä¾†è‡ª N8N çš„å¸‚å ´åˆ†æè³‡æ–™ - å¢å¼·ç‰ˆæœ¬"""
    try:
        global stored_data, system_stats

        raw_data = await request.json()
        logger.info(f"ğŸ“¨ æ”¶åˆ° N8N åŸå§‹è³‡æ–™å¤§å°: {len(json.dumps(raw_data, ensure_ascii=False))} å­—å…ƒ")
        logger.info(f"ğŸ“¨ æ”¶åˆ° N8N è³‡æ–™: {json.dumps(raw_data, ensure_ascii=False)[:500]}...")

        # å¢å¼·çš„æ•¸æ“šè™•ç†é‚è¼¯
        if isinstance(raw_data, list) and len(raw_data) > 0:
            market_data = raw_data[0]
            logger.info("âœ… è™•ç†é™£åˆ—æ ¼å¼æ•¸æ“šï¼Œå–ç¬¬ä¸€å€‹å…ƒç´ ")
        elif isinstance(raw_data, dict):
            market_data = raw_data
            logger.info("âœ… è™•ç†å­—å…¸æ ¼å¼æ•¸æ“š")
        else:
            logger.error(f"âŒ ç„¡æ•ˆçš„è³‡æ–™æ ¼å¼: {type(raw_data)}")
            raise HTTPException(status_code=400, detail=f"ç„¡æ•ˆçš„è³‡æ–™æ ¼å¼: {type(raw_data)}")

        # è©³ç´°è¨˜éŒ„æ¥æ”¶åˆ°çš„æ•¸æ“šæ¬„ä½
        logger.info(f"ğŸ“Š æ•¸æ“šæ¬„ä½: {list(market_data.keys())}")

        # æ§‹å»ºå„²å­˜çš„æ•¸æ“š
        current_time = datetime.now()

        # è™•ç†emailReportå…§å®¹
        email_report = ""
        if "data" in market_data and isinstance(market_data["data"], dict):
            email_report = market_data["data"].get("emailReport", "")
            logger.info(f"ğŸ“§ æ‰¾åˆ°emailReportå…§å®¹ï¼Œé•·åº¦: {len(email_report)} å­—å…ƒ")
        elif "emailReport" in market_data:
            email_report = market_data.get("emailReport", "")
            logger.info(f"ğŸ“§ ç›´æ¥æ‰¾åˆ°emailReportå…§å®¹ï¼Œé•·åº¦: {len(email_report)} å­—å…ƒ")

        stored_data = {
            "positive": int(market_data.get("positive", 0)),
            "neutral": int(market_data.get("neutral", 0)),
            "negative": int(market_data.get("negative", 0)),
            "summary": str(market_data.get("summary", "")),
            "score": int(market_data.get("score", 0)),
            "label": str(market_data.get("label", "")),
            "emailReportHtml": str(market_data.get("emailReportHtml", "")),
            "received_time": current_time.strftime("%Y-%m-%d %H:%M:%S"),
            "received_timestamp": current_time.isoformat(),
            "raw_data": market_data,
            "email_report": email_report,  # æ–°å¢emailReportæ¬„ä½
            "data_source": "N8N Webhook",
            "processing_time": datetime.now().isoformat()
        }

        # æ›´æ–°ç³»çµ±çµ±è¨ˆ
        system_stats["total_reports"] += 1
        system_stats["today_reports"] += 1
        system_stats["last_data_received"] = current_time.isoformat()

        # è©³ç´°è¨˜éŒ„è™•ç†å¾Œçš„æ•¸æ“š
        # logger.info(f"âœ… æˆåŠŸè™•ç† N8N è³‡æ–™:")
        # logger.info(f"   æƒ…æ„Ÿåˆ†æ•¸: {stored_data['average_sentiment_score']}")
        # logger.info(f"   å…§å®¹é•·åº¦: {len(stored_data['message_content'])} å­—å…ƒ")
        # logger.info(f"   å¸‚å ´æ—¥æœŸ: {stored_data['market_date']}")
        # logger.info(f"   ä¿¡å¿ƒæ°´å¹³: {stored_data['confidence_level']}")
        # logger.info(f"   è¶¨å‹¢æ–¹å‘: {stored_data['trend_direction']}")
        # logger.info(f"   é¢¨éšªè©•ä¼°: {stored_data['risk_assessment']}")
        # logger.info(f"   æ¥æ”¶æ™‚é–“: {stored_data['received_time']}")

        return {
            "status": "success",
            "message": "å¸‚å ´åˆ†æè³‡æ–™å·²æ¥æ”¶ä¸¦å„²å­˜",
            "data": stored_data,
            "received_at": current_time.isoformat(),
            "processed_fields": len(stored_data),
            "system_stats": system_stats
        }

    except ValueError as ve:
        logger.error(f"âŒ æ•¸æ“šé©—è­‰éŒ¯èª¤: {str(ve)}")
        system_stats["errors"] += 1
        raise HTTPException(status_code=400, detail=f"æ•¸æ“šé©—è­‰éŒ¯èª¤: {str(ve)}")
    except Exception as e:
        logger.error(f"âŒ æ¥æ”¶ N8N è³‡æ–™å¤±æ•—: {str(e)}")
        system_stats["errors"] += 1
        raise HTTPException(status_code=500, detail=f"æ¥æ”¶è³‡æ–™å¤±æ•—: {str(e)}")


@app.get("/api/current-data")
async def get_current_data():
    """å–å¾—ç›®å‰å„²å­˜çš„å¸‚å ´åˆ†æè³‡æ–™ - å¢å¼·ç‰ˆæœ¬"""
    try:
        system_stats["api_calls"] += 1

        # logger.info(f"ğŸ“¤ API è«‹æ±‚ /api/current-data")
        # logger.info(f"ğŸ“Š ç•¶å‰å„²å­˜æ•¸æ“šç‹€æ…‹: {'æœ‰æ•¸æ“š' if stored_data else 'ç„¡æ•¸æ“š'}")

        # if stored_data:
        # logger.info(f"ğŸ“Š æ•¸æ“šè©³æƒ…:")
        # logger.info(f"   æƒ…æ„Ÿåˆ†æ•¸: {stored_data.get('average_sentiment_score', 'N/A')}")
        # logger.info(f"   å…§å®¹é•·åº¦: {len(stored_data.get('message_content', ''))} å­—å…ƒ")
        # logger.info(f"   æ¥æ”¶æ™‚é–“: {stored_data.get('received_time', 'N/A')}")

        # æª¢æŸ¥æ•¸æ“šæ˜¯å¦éæœŸï¼ˆè¶…é1å°æ™‚ï¼‰
        data_age_minutes = 0
        if stored_data and stored_data.get('received_timestamp'):
            try:
                received_time = datetime.fromisoformat(stored_data['received_timestamp'])
                data_age_minutes = (datetime.now() - received_time).total_seconds() / 60
                # logger.info(f"ğŸ“… æ•¸æ“šå¹´é½¡: {data_age_minutes:.1f} åˆ†é˜")
            except Exception as e:
                logger.warning(f"âš ï¸ ç„¡æ³•è¨ˆç®—æ•¸æ“šå¹´é½¡: {e}")

        response_data = {
            "status": "success",
            "data": stored_data,
            "stats": system_stats,
            "timestamp": datetime.now().isoformat(),
            "has_data": len(stored_data) > 0,
            "data_age_minutes": data_age_minutes,
            "data_freshness": "fresh" if data_age_minutes < 60 else "stale" if data_age_minutes < 1440 else "very_old"
        }

        # logger.info(f"âœ… å›å‚³æ•¸æ“š: {len(stored_data)} å€‹æ¬„ä½")
        return response_data

    except Exception as e:
        logger.error(f"âŒ å–å¾—ç•¶å‰æ•¸æ“šå¤±æ•—: {str(e)}")
        system_stats["errors"] += 1
        raise HTTPException(status_code=500, detail=f"å–å¾—æ•¸æ“šå¤±æ•—: {str(e)}")


@app.get("/api/gold-price")
async def get_gold_price(period: str = "1y", interval: str = "1d"):
    """å–å¾—é»ƒé‡‘æœŸè²¨åƒ¹æ ¼ - å¢å¼·ç‰ˆæœ¬"""
    try:
        system_stats["gold_price_calls"] += 1

        # é©—è­‰åƒæ•¸
        valid_periods = ["1d", "5d", "1mo", "3mo", "6mo", "1y", "2y", "5y"]
        valid_intervals = ["1m", "5m", "15m", "30m", "1h", "1d"]

        if period not in valid_periods:
            logger.warning(f"ç„¡æ•ˆçš„æ™‚é–“æœŸé–“: {period}ï¼Œä½¿ç”¨é è¨­å€¼ 1y")
            period = "1y"

        if interval not in valid_intervals:
            logger.warning(f"ç„¡æ•ˆçš„æ™‚é–“é–“éš”: {interval}ï¼Œä½¿ç”¨é è¨­å€¼ 1d")
            interval = "1d"

        # logger.info(f"ğŸ” é–‹å§‹ç²å–é»ƒé‡‘æœŸè²¨æ•¸æ“š")
        # logger.info(f"   æœŸé–“: {period}")
        # logger.info(f"   é–“éš”: {interval}")
        # logger.info(f"   è«‹æ±‚æ™‚é–“: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")

        # ç²å–é»ƒé‡‘æœŸè²¨æ•¸æ“š
        try:
            hist_data, info, current_price, latest_processing_time = await get_gold_futures_data_enhanced(period,
                                                                                                          interval)

            if hist_data is None or hist_data.empty:
                logger.warning("âš ï¸ ä¸»è¦æ•¸æ“šæºç„¡æ•¸æ“šï¼Œä½¿ç”¨å‚™é¸æ–¹æ¡ˆ...")
                return create_mock_gold_data(period)

        except Exception as e:
            logger.error(f"âŒ yfinance æ•¸æ“šç²å–å¤±æ•—: {str(e)}")
            return create_mock_gold_data(period)

        # logger.info(f"âœ… æˆåŠŸç²å–é»ƒé‡‘æ•¸æ“š:")
        # logger.info(f"   æ•¸æ“šé»æ•¸é‡: {len(hist_data)}")
        # logger.info(
        #     f"   æ—¥æœŸç¯„åœ: {hist_data.index[0].strftime('%Y-%m-%d')} åˆ° {hist_data.index[-1].strftime('%Y-%m-%d')}")

        # è¨ˆç®—çµ±è¨ˆæ•¸æ“š
        stats = calculate_gold_statistics(hist_data)

        if not stats:
            logger.warning("âš ï¸ çµ±è¨ˆè¨ˆç®—å¤±æ•—ï¼Œä½¿ç”¨å‚™é¸æ•¸æ“š")
            return create_mock_gold_data(period)

        # logger.info(f"ğŸ’° åƒ¹æ ¼çµ±è¨ˆ:")
        # logger.info(f"   ç•¶å‰åƒ¹æ ¼: ${stats['current_price']:.2f}")
        # logger.info(f"   åƒ¹æ ¼è®ŠåŒ–: ${stats['price_change']:+.2f} ({stats['price_change_pct']:+.2f}%)")
        # logger.info(f"   åƒ¹æ ¼ç¯„åœ: ${stats['min_price']:.2f} - ${stats['max_price']:.2f}")
        # logger.info(f"   å¹³å‡åƒ¹æ ¼: ${stats['avg_price']:.2f}")
        # logger.info(f"   æ³¢å‹•ç‡: {stats['volatility']:.2f}")

        # æº–å‚™åœ–è¡¨æ•¸æ“š
        chart_data = []
        for idx, row in hist_data.iterrows():
            try:
                # ä¿®æ­£æ™‚å€å•é¡Œï¼Œè½‰æ›ç‚ºå°åŒ—æ™‚é–“
                if hasattr(idx, 'tz_localize'):
                    if idx.tz is None:
                        # å‡è¨­æ˜¯UTCæ™‚é–“ï¼Œè½‰æ›ç‚ºå°åŒ—æ™‚é–“
                        idx_local = idx + timedelta(hours=8)
                    else:
                        # è½‰æ›ç‚ºå°åŒ—æ™‚é–“
                        idx_local = idx.tz_convert('Asia/Taipei')
                else:
                    # å‡è¨­æ˜¯UTCæ™‚é–“ï¼Œè½‰æ›ç‚ºå°åŒ—æ™‚é–“
                    idx_local = idx + timedelta(hours=8)

                data_point = {
                    "time": idx_local.strftime('%Y-%m-%d'),
                    "price": float(row['Close']) if not pd.isna(row['Close']) else stats['current_price'],
                    "high": float(row['High']) if not pd.isna(row['High']) else stats['current_price'],
                    "low": float(row['Low']) if not pd.isna(row['Low']) else stats['current_price'],
                    "open": float(row['Open']) if not pd.isna(row['Open']) else stats['current_price'],
                    "volume": int(row['Volume']) if not pd.isna(row['Volume']) and row['Volume'] > 0 else 0
                }
                logger.debug(f"æ•¸æ“šé»: {data_point['time']} - ${data_point['price']:.2f}")
                chart_data.append(data_point)
            except Exception as point_error:
                logger.warning(f"âš ï¸ è™•ç†æ•¸æ“šé»æ™‚å‡ºéŒ¯: {point_error}")
                continue

        # logger.info(f"ğŸ“Š åœ–è¡¨æ•¸æ“š:")
        # logger.info(f"   æœ‰æ•ˆæ•¸æ“šé»: {len(chart_data)}")
        if chart_data:
            prices = [d['price'] for d in chart_data]
            valid_prices = [p for p in prices if not pd.isna(p) and p > 0]
            # logger.info(f"   æœ‰æ•ˆåƒ¹æ ¼æ•¸é‡: {len(valid_prices)}")
            # if valid_prices:
            #     logger.info(f"   åƒ¹æ ¼ç¯„åœ: ${min(valid_prices):.2f} - ${max(valid_prices):.2f}")
            # logger.info(f"   å‰3å€‹æ•¸æ“šé»: {chart_data[:3]}")

        # è¨ˆç®—æŠ€è¡“æŒ‡æ¨™
        technical_indicators = calculate_technical_indicators_enhanced(hist_data)
        # logger.info(f"ğŸ“ˆ æŠ€è¡“æŒ‡æ¨™: {list(technical_indicators.keys())}")

        # è¨ˆç®—ç§»å‹•å¹³å‡ç·šæ•¸æ“š
        ma_lines = {}
        if len(hist_data) >= 5:
            ma_5_data = hist_data['Close'].rolling(window=5).mean().dropna()
            ma_5_line_data = []
            for idx, val in ma_5_data.items():
                # ç¢ºä¿æ™‚é–“æ ¼å¼èˆ‡åœ–è¡¨æ•¸æ“šä¸€è‡´ï¼Œä¸¦æ­£ç¢ºè™•ç†æ™‚å€
                if hasattr(idx, 'tz_localize'):
                    if idx.tz is None:
                        # å‡è¨­æ˜¯UTCæ™‚é–“ï¼Œè½‰æ›ç‚ºå°åŒ—æ™‚é–“
                        idx_local = idx + timedelta(hours=8)
                    else:
                        # è½‰æ›ç‚ºå°åŒ—æ™‚é–“
                        idx_local = idx.tz_convert('Asia/Taipei')
                else:
                    # å‡è¨­æ˜¯UTCæ™‚é–“ï¼Œè½‰æ›ç‚ºå°åŒ—æ™‚é–“
                    idx_local = idx + timedelta(hours=8)

                ma_5_line_data.append({
                    'time': idx_local.strftime('%Y-%m-%d'),
                    'price': float(val)
                })
            ma_lines["ma_5"] = ma_5_line_data

        if len(hist_data) >= 20:
            ma_20_data = hist_data['Close'].rolling(window=20).mean().dropna()
            ma_20_line_data = []
            for idx, val in ma_20_data.items():
                # ç¢ºä¿æ™‚é–“æ ¼å¼èˆ‡åœ–è¡¨æ•¸æ“šä¸€è‡´ï¼Œä¸¦æ­£ç¢ºè™•ç†æ™‚å€
                if hasattr(idx, 'tz_localize'):
                    if idx.tz is None:
                        # å‡è¨­æ˜¯UTCæ™‚é–“ï¼Œè½‰æ›ç‚ºå°åŒ—æ™‚é–“
                        idx_local = idx + timedelta(hours=8)
                    else:
                        # è½‰æ›ç‚ºå°åŒ—æ™‚é–“
                        idx_local = idx.tz_convert('Asia/Taipei')
                else:
                    # å‡è¨­æ˜¯UTCæ™‚é–“ï¼Œè½‰æ›ç‚ºå°åŒ—æ™‚é–“
                    idx_local = idx + timedelta(hours=8)

                ma_20_line_data.append({
                    'time': idx_local.strftime('%Y-%m-%d'),
                    'price': float(val)
                })
            ma_lines["ma_20"] = ma_20_line_data

        # è¨ˆç®—MA125ç·šï¼ˆæ›¿ä»£æœˆå¹³å‡ç·šï¼‰
        ma_125_line = calculate_ma125_line(hist_data)

        # è¨ˆç®—å­£å¹³å‡åƒ¹æ ¼ç·šï¼ˆæ›¿ä»£å¹´å¹³å‡ç·šï¼‰
        quarterly_average_line = calculate_quarterly_average_line(hist_data)

        # æª¢æ¸¬é»ƒé‡‘äº¤å‰å’Œæ­»äº¡äº¤å‰
        cross_signal = detect_golden_death_cross(hist_data)

        # åˆ¤æ–·å¸‚å ´ç‹€æ…‹
        market_status = determine_market_status()
        # logger.info(f"ğŸª å¸‚å ´ç‹€æ…‹: {market_status}")

        # ç²å–å¸‚å ´è³‡è¨Š
        market_name = get_market_name(info)

        # è¨ˆç®—ç•¶æ—¥é«˜å’Œç•¶æ—¥ä½
        today_high = None
        today_low = None
        try:
            # ç²å–ç•¶å¤©çš„æ•¸æ“š
            today = datetime.now().date()
            today_data = hist_data[hist_data.index.date == today]
            if not today_data.empty:
                today_high = float(today_data['High'].max())
                today_low = float(today_data['Low'].min())
                logger.info(f"ğŸ“Š ç•¶æ—¥æ•¸æ“š: é«˜=${today_high:.2f}, ä½=${today_low:.2f}")
            else:
                # å¦‚æœæ²’æœ‰ç•¶å¤©æ•¸æ“šï¼Œä½¿ç”¨æœ€è¿‘ä¸€å¤©çš„æ•¸æ“š
                if len(hist_data) > 0:
                    latest_data = hist_data.iloc[-1]
                    today_high = float(latest_data['High'])
                    today_low = float(latest_data['Low'])
                    logger.info(f"ğŸ“Š ä½¿ç”¨æœ€è¿‘æ•¸æ“š: é«˜=${today_high:.2f}, ä½=${today_low:.2f}")
        except Exception as e:
            logger.warning(f"âš ï¸ è¨ˆç®—ç•¶æ—¥é«˜ä½åƒ¹å¤±æ•—: {e}")
            today_high = stats['current_price']
            today_low = stats['current_price']

        # æº–å‚™å›æ‡‰æ•¸æ“š
        response_data = {
            "status": "success",
            "data": {
                "symbol": "GC=F",
                "name": market_name,
                "current_price": round(stats['current_price'], 2),
                "change": round(stats['price_change'], 2),
                "change_percent": round(stats['price_change_pct'], 2),
                "high_24h": round(stats['max_price'], 2),
                "low_24h": round(stats['min_price'], 2),
                "today_high": round(today_high, 2) if today_high else None,
                "today_low": round(today_low, 2) if today_low else None,
                "avg_price": round(stats['avg_price'], 2),
                "volatility": round(stats['volatility'], 2),
                "volume_24h": 0,  # ç§»é™¤äº¤æ˜“é‡é¡¯ç¤º
                "currency": "USD",
                "unit": "per ounce",
                "last_updated": stats['latest_date'].isoformat(),
                "last_updated_formatted": latest_processing_time,
                "chart_data": chart_data,
                "ma_lines": ma_lines,
                "ma_125_line": ma_125_line,
                "pivot_points": quarterly_average_line,  # è½‰æŠ˜é»æ•¸æ“š
                "cross_signal": cross_signal,
                "market_status": market_status,
                "technical_indicators": technical_indicators,
                "period": period,
                "interval": interval,
                "data_points": len(chart_data),
                "trading_days": len(hist_data),
                "data_source_info": {
                    "primary": "Yahoo Finance",
                    "realtime_updated": len(chart_data) > 0 and
                                        chart_data[-1]['time'].split('T')[0] == datetime.now().strftime('%Y-%m-%d')
                }
            },
            "system_time": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
            "next_update": (datetime.now() + timedelta(minutes=5)).strftime("%Y-%m-%d %H:%M:%S"),
            "data_source": "Yahoo Finance API (Enhanced)",
            "processing_stats": {
                "raw_data_points": len(hist_data),
                "processed_chart_points": len(chart_data),
                "technical_indicators_count": len(technical_indicators),
                "processing_time": datetime.now().isoformat()
            }
        }

        # logger.info(f"âœ… æˆåŠŸå›å‚³é»ƒé‡‘åƒ¹æ ¼æ•¸æ“š:")
        # logger.info(f"   å›æ‡‰å¤§å°: {len(json.dumps(response_data))} å­—å…ƒ")
        # logger.info(f"   åœ–è¡¨æ•¸æ“šé»: {len(chart_data)}")

        return response_data

    except Exception as e:
        logger.error(f"âŒ ç²å–é»ƒé‡‘åƒ¹æ ¼å¤±æ•—: {str(e)}")
        system_stats["errors"] += 1
        return create_mock_gold_data(period)


async def get_gold_futures_data_enhanced(period: str, interval: str):
    """ç²å–é»ƒé‡‘æœŸè²¨æ•¸æ“š - å¢å¼·ç‰ˆæœ¬"""
    try:
        # è¨ˆç®—æ™‚é–“ç¯„åœ
        period_days_map = {
            '1d': 1, '5d': 5, '1mo': 30, '3mo': 90,
            '6mo': 180, '1y': 365, '2y': 730, '5y': 1825
        }
        period_days = period_days_map.get(period, 365)

        end_date = datetime.now()
        start_date = end_date - timedelta(days=period_days)

        # logger.info(f"ğŸ• æ™‚é–“ç¯„åœ: {start_date.strftime('%Y-%m-%d')} è‡³ {end_date.strftime('%Y-%m-%d')}")

        # ä½¿ç”¨yfinanceç²å–æ•¸æ“š
        gold_ticker = yf.Ticker("GC=F")

        # ç²å–æ­·å²æ•¸æ“š
        # logger.info(f"ğŸ“Š æ­£åœ¨ç²å–æ­·å²æ•¸æ“š...")
        hist_data = gold_ticker.history(
            start=start_date.strftime('%Y-%m-%d'),
            end=end_date.strftime('%Y-%m-%d'),
            interval=interval
        )

        # logger.info(f"ğŸ“Š ç²å–åˆ° {len(hist_data)} ç­†æ­·å²æ•¸æ“š")

        # å˜—è©¦ç²å–ç•¶å¤©çš„åˆ†é˜ç´šæ•¸æ“š
        try:
            # logger.info("ğŸ”„ æ­£åœ¨ç²å–ç•¶å¤©è©³ç´°æ•¸æ“š...")
            recent_data = gold_ticker.history(
                period='2d',
                interval='1m'
            )

            if not recent_data.empty:
                today = datetime.now().date()
                today_data = recent_data[recent_data.index.date >= today]

                if not today_data.empty:
                    latest_price = today_data['Close'].iloc[-1]
                    latest_time = today_data.index[-1]

                    # logger.info(f"ğŸ“Š ä»Šæ—¥æ•¸æ“š: {len(today_data)} ç­†ï¼Œæœ€æ–°åƒ¹æ ¼: ${latest_price:.2f}")
                    # logger.info(f"ğŸ“… åŸå§‹æ™‚é–“: {latest_time}")

                    # æ›´æ–°æ­·å²æ•¸æ“šä¸­çš„æœ€æ–°åƒ¹æ ¼
                    if len(hist_data) > 0:
                        last_date = hist_data.index[-1].date()
                        if last_date == today:
                            # æ›´æ–°ä»Šå¤©çš„æ•¸æ“š
                            hist_data.loc[hist_data.index[-1], 'Close'] = latest_price
                            hist_data.loc[hist_data.index[-1], 'High'] = max(
                                hist_data.loc[hist_data.index[-1], 'High'], latest_price
                            )
                            hist_data.loc[hist_data.index[-1], 'Low'] = min(
                                hist_data.loc[hist_data.index[-1], 'Low'], latest_price
                            )
                            # logger.info("âœ… å·²æ›´æ–°ä»Šæ—¥æ•¸æ“š")
                        else:
                            # æ·»åŠ ä»Šå¤©çš„æ•¸æ“š
                            new_row = pd.DataFrame({
                                'Open': [today_data['Open'].iloc[0]],
                                'High': [today_data['High'].max()],
                                'Low': [today_data['Low'].min()],
                                'Close': [latest_price],
                                'Volume': [today_data['Volume'].sum()]
                            }, index=[latest_time.replace(hour=0, minute=0, second=0, microsecond=0)])
                            hist_data = pd.concat([hist_data, new_row])
                            # logger.info("âœ… å·²æ·»åŠ ä»Šæ—¥æ•¸æ“š")

                    # ä¿®æ­£æ™‚å€å•é¡Œï¼Œè½‰æ›ç‚ºå°åŒ—æ™‚é–“ (+8)
                    if hasattr(latest_time, 'tz_localize'):
                        # å¦‚æœæ˜¯æ™‚å€æ„ŸçŸ¥çš„æ™‚é–“ï¼Œè½‰æ›ç‚ºå°åŒ—æ™‚é–“
                        if latest_time.tz is None:
                            # å‡è¨­æ˜¯UTCæ™‚é–“ï¼Œè½‰æ›ç‚ºå°åŒ—æ™‚é–“
                            latest_time_local = latest_time + timedelta(hours=8)
                        else:
                            # è½‰æ›ç‚ºå°åŒ—æ™‚é–“
                            latest_time_local = latest_time.tz_convert('Asia/Taipei')
                    else:
                        # å‡è¨­æ˜¯UTCæ™‚é–“ï¼Œè½‰æ›ç‚ºå°åŒ—æ™‚é–“
                        latest_time_local = latest_time + timedelta(hours=8)

                    latest_time_formatted = latest_time_local.strftime('%Y-%m-%d %H:%M')
                    # logger.info(f"âœ… ç•¶å¤©æ•¸æ“šè™•ç†å®Œæˆï¼Œæœ€æ–°æ™‚é–“: {latest_time_formatted}")
                else:
                    logger.info("â„¹ï¸ ç•¶å¤©æš«ç„¡äº¤æ˜“æ•¸æ“š")
            else:
                logger.info("â„¹ï¸ ç„¡æ³•ç²å–ç•¶å¤©è©³ç´°æ•¸æ“š")

        except Exception as e:
            logger.warning(f"âš ï¸ ç²å–ç•¶å¤©æ•¸æ“šæ™‚å‡ºç¾å•é¡Œ: {e}")

        if hist_data.empty:
            raise ValueError("ç„¡æ³•ç²å–æ•¸æ“šï¼Œè«‹æª¢æŸ¥ç¶²è·¯é€£æ¥æˆ–APIç‹€æ…‹")

        # ç²å–å¸‚å ´è³‡è¨Š
        info = None
        try:
            info = gold_ticker.info
            # logger.info(f"ğŸ“‹ ç²å–å¸‚å ´è³‡è¨Š: {info.get('longName', 'N/A') if info else 'N/A'}")
        except Exception as info_error:
            logger.warning(f"âš ï¸ ç„¡æ³•ç²å–å¸‚å ´è³‡è¨Š: {info_error}")

        current_price = hist_data['Close'].iloc[-1] if not hist_data.empty else None

        # logger.info(f"âœ… æ•¸æ“šç²å–å®Œæˆ:")
        # logger.info(f"   æœ€çµ‚æ•¸æ“šé»æ•¸: {len(hist_data)}")
        # logger.info(f"   æœ€æ–°åƒ¹æ ¼: ${current_price:.2f}")
        # logger.info(f"   æœ€å¾Œæ›´æ–°: {hist_data.index[-1].strftime('%Y-%m-%d %H:%M')}")

        # ç²å–æœ€æ–°çš„è™•ç†æ™‚é–“
        latest_processing_time = None
        if 'latest_time_formatted' in locals():
            latest_processing_time = latest_time_formatted
        else:
            # ä¿®æ­£æ™‚å€å•é¡Œï¼Œè½‰æ›ç‚ºå°åŒ—æ™‚é–“ (+8)
            last_time = hist_data.index[-1]
            if hasattr(last_time, 'tz_localize'):
                if last_time.tz is None:
                    # å‡è¨­æ˜¯UTCæ™‚é–“ï¼Œè½‰æ›ç‚ºå°åŒ—æ™‚é–“
                    last_time_local = last_time + timedelta(hours=8)
                else:
                    # è½‰æ›ç‚ºå°åŒ—æ™‚é–“
                    last_time_local = last_time.tz_convert('Asia/Taipei')
            else:
                # å‡è¨­æ˜¯UTCæ™‚é–“ï¼Œè½‰æ›ç‚ºå°åŒ—æ™‚é–“
                last_time_local = last_time + timedelta(hours=8)
            latest_processing_time = last_time_local.strftime('%Y-%m-%d %H:%M')

        return hist_data, info, current_price, latest_processing_time

    except Exception as e:
        logger.error(f"âŒ ç²å–æ•¸æ“šæ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
        return None, None, None


def calculate_gold_statistics(data):
    """è¨ˆç®—é»ƒé‡‘çµ±è¨ˆæ•¸æ“š"""
    if data is None or data.empty:
        return {}

    try:
        close_prices = data['Close']

        # è¨ˆç®—æ—¥è®ŠåŒ–ï¼ˆç•¶å‰åƒ¹æ ¼èˆ‡æ˜¨å¤©æ”¶ç›¤åƒ¹æ ¼çš„å·®é¡ï¼‰
        current_price = float(close_prices.iloc[-1])
        yesterday_price = float(close_prices.iloc[-2]) if len(close_prices) > 1 else current_price

        daily_change = current_price - yesterday_price
        daily_change_pct = ((daily_change / yesterday_price) * 100) if yesterday_price != 0 else 0

        # è¨ˆç®—å¹´åº¦æ¨™æº–å·®ï¼ˆä½¿ç”¨æ•´å€‹æ•¸æ“šé›†ï¼‰
        # ä½¿ç”¨ ddof=1 ä¾†è¨ˆç®—æ¨£æœ¬æ¨™æº–å·®ï¼Œèˆ‡å‰ç«¯çš„è¨ˆç®—æ–¹æ³•ä¿æŒä¸€è‡´
        annual_volatility = float(close_prices.std(ddof=1))

        stats = {
            'current_price': current_price,
            'max_price': float(close_prices.max()),
            'min_price': float(close_prices.min()),
            'avg_price': float(close_prices.mean()),
            'price_change': daily_change,  # æ—¥è®ŠåŒ–
            'price_change_pct': daily_change_pct,  # æ—¥è®ŠåŒ–ç™¾åˆ†æ¯”
            'volatility': annual_volatility,  # å¹´åº¦æ¨™æº–å·®
            'latest_date': close_prices.index[-1],
            'yesterday_price': yesterday_price  # æ·»åŠ æ˜¨å¤©åƒ¹æ ¼ç”¨æ–¼èª¿è©¦
        }

        # logger.info(f"ğŸ“Š çµ±è¨ˆè¨ˆç®—å®Œæˆ: ç•¶å‰=${stats['current_price']:.2f}, æ—¥è®ŠåŒ–={stats['price_change']:+.2f}")
        return stats
    except Exception as e:
        logger.error(f"âŒ çµ±è¨ˆè¨ˆç®—å¤±æ•—: {e}")
        return {}


def calculate_technical_indicators_enhanced(hist_data):
    """è¨ˆç®—æŠ€è¡“æŒ‡æ¨™ - å¢å¼·ç‰ˆæœ¬"""
    technical_indicators = {}

    try:
        close_prices = hist_data['Close'].dropna()

        # MA5 è¨ˆç®—
        if len(close_prices) >= 5:
            ma_5 = close_prices.rolling(window=5).mean().iloc[-1]
            if not pd.isna(ma_5):
                technical_indicators["ma_5"] = float(ma_5)

        # MA20 è¨ˆç®—
        if len(close_prices) >= 20:
            ma_20 = close_prices.rolling(window=20).mean().iloc[-1]
            if not pd.isna(ma_20):
                technical_indicators["ma_20"] = float(ma_20)

        # MA50 è¨ˆç®—
        if len(close_prices) >= 50:
            ma_50 = close_prices.rolling(window=50).mean().iloc[-1]
            if not pd.isna(ma_50):
                technical_indicators["ma_50"] = float(ma_50)

        # RSI è¨ˆç®—
        if len(close_prices) >= 14:
            rsi = calculate_rsi(close_prices.values)
            if rsi is not None:
                technical_indicators["rsi"] = rsi

        # é¡å¤–æŠ€è¡“æŒ‡æ¨™
        if len(close_prices) >= 5:
            technical_indicators["volatility_5d"] = float(close_prices.tail(5).std())

        if len(close_prices) >= 20 and "ma_20" in technical_indicators:
            technical_indicators["price_vs_ma20"] = float(
                (close_prices.iloc[-1] / technical_indicators["ma_20"] - 1) * 100)

        # logger.info(f"ğŸ“ˆ æŠ€è¡“æŒ‡æ¨™è¨ˆç®—å®Œæˆ: {len(technical_indicators)} å€‹æŒ‡æ¨™")

    except Exception as e:
        logger.warning(f"âš ï¸ æŠ€è¡“æŒ‡æ¨™è¨ˆç®—éŒ¯èª¤: {e}")

    return technical_indicators


def calculate_rsi(prices, periods=14):
    """è¨ˆç®— RSI æŠ€è¡“æŒ‡æ¨™"""
    try:
        if len(prices) < periods + 1:
            return None

        prices = np.array(prices, dtype=float)
        prices = prices[~np.isnan(prices)]

        if len(prices) < periods + 1:
            return None

        deltas = np.diff(prices)

        if len(deltas) < periods:
            return None

        up_moves = np.where(deltas > 0, deltas, 0)
        down_moves = np.where(deltas < 0, -deltas, 0)

        if len(up_moves) >= periods and len(down_moves) >= periods:
            avg_up = np.mean(up_moves[-periods:])
            avg_down = np.mean(down_moves[-periods:])

            if avg_down == 0:
                return 100.0

            rs = avg_up / avg_down
            rsi = 100 - (100 / (1 + rs))
            return round(float(rsi), 1)

        return None

    except Exception as e:
        logger.warning(f"âš ï¸ RSI è¨ˆç®—éŒ¯èª¤: {e}")
        return None


def calculate_monthly_average_line(hist_data):
    """è¨ˆç®—æ¯æœˆæœ€é«˜æœ€ä½åƒ¹æ ¼å¹³å‡å€¼çš„ç·š - æ¯å€‹æœˆä¸€å€‹é»"""
    try:
        # ç¢ºä¿æ•¸æ“šæœ‰æ—¥æœŸç´¢å¼•
        if not isinstance(hist_data.index, pd.DatetimeIndex):
            hist_data.index = pd.to_datetime(hist_data.index)

        # çµ±ä¸€æ™‚å€è™•ç† - è½‰æ›ç‚ºç„¡æ™‚å€çš„æ—¥æœŸ
        hist_data.index = hist_data.index.tz_localize(None)

        # æŒ‰æœˆä»½åˆ†çµ„ä¸¦è¨ˆç®—æ¯æœˆçš„æœ€é«˜å’Œæœ€ä½åƒ¹æ ¼
        monthly_data = hist_data.groupby(hist_data.index.to_period('M')).agg({
            'High': 'max',
            'Low': 'min'
        })

        # è¨ˆç®—æ¯æœˆæœ€é«˜æœ€ä½åƒ¹æ ¼çš„å¹³å‡å€¼
        monthly_averages = (monthly_data['High'] + monthly_data['Low']) / 2

        # å–æœ€è¿‘12å€‹æœˆçš„æ•¸æ“š
        monthly_averages = monthly_averages.tail(12)

        # è½‰æ›ç‚ºåœ–è¡¨æ•¸æ“šæ ¼å¼ - æ¯å€‹æœˆåªå‰µå»ºä¸€å€‹æ•¸æ“šé»
        monthly_line_data = []
        for period, avg_price in monthly_averages.items():
            try:
                # ä½¿ç”¨è©²æœˆçš„æœ€å¾Œä¸€å€‹äº¤æ˜“æ—¥ä½œç‚ºä»£è¡¨æ—¥æœŸ
                try:
                    month_end = period.to_timestamp() + pd.offsets.MonthEnd(0)
                except Exception as e:
                    logger.warning(f"âš ï¸ è½‰æ›æœˆä»½ {period} æ™‚å‡ºéŒ¯: {e}")
                    continue

                # æ‰¾åˆ°è©²æœˆçš„æœ€å¾Œä¸€å€‹äº¤æ˜“æ—¥
                month_trading_days = [date for date in hist_data.index if date <= month_end]
                if month_trading_days:
                    last_trading_day = max(month_trading_days)
                    monthly_line_data.append({
                        'time': str(last_trading_day)[:10],  # å–å‰10å€‹å­—ç¬¦ä½œç‚ºæ—¥æœŸ
                        'price': float(avg_price)
                    })
            except Exception as e:
                logger.warning(f"âš ï¸ è™•ç†æœˆä»½ {period} æ™‚å‡ºéŒ¯: {e}")
                continue

        logger.info(f"ğŸ“Š æœˆå¹³å‡ç·šè¨ˆç®—å®Œæˆï¼Œå…± {len(monthly_line_data)} å€‹æ•¸æ“šé»")
        logger.info(f"    æœˆå¹³å‡åƒ¹æ ¼ç¯„åœ: ${monthly_averages.min():.2f} - ${monthly_averages.max():.2f}")

        return monthly_line_data

    except Exception as e:
        logger.warning(f"âš ï¸ æ¯æœˆå¹³å‡ç·šè¨ˆç®—éŒ¯èª¤: {e}")
        return []


def calculate_quarterly_average_line(hist_data):
    """
    è¨ˆç®—è½‰æŠ˜é»ï¼ˆPivot Pointï¼‰- æ¯æœˆåˆè¨ˆç®—ä¸€æ¬¡ï¼Œè©²é»ç‚ºå‰ä¸‰å€‹æœˆæœ€é«˜åƒ¹èˆ‡æœ€ä½åƒ¹çš„å¹³å‡å€¼ï¼Œæ¯æœˆåªç”¢ç”Ÿä¸€å€‹é»ï¼Œä¸¦å¯é€£æˆæŠ˜ç·šåœ–ã€‚
    """
    try:
        # ç¢ºä¿ç´¢å¼•ç‚º DatetimeIndex
        if not isinstance(hist_data.index, pd.DatetimeIndex):
            hist_data.index = pd.to_datetime(hist_data.index)

        # çµ±ä¸€è™•ç†æ™‚å€å•é¡Œï¼Œè½‰æ›ç‚ºå°åŒ—æ™‚é–“ (+8)
        if hist_data.index.tz is None:
            # å‡è¨­æ˜¯UTCæ™‚é–“ï¼Œè½‰æ›ç‚ºå°åŒ—æ™‚é–“
            hist_data.index = hist_data.index + timedelta(hours=8)
        else:
            # è½‰æ›ç‚ºå°åŒ—æ™‚é–“
            hist_data.index = hist_data.index.tz_convert('Asia/Taipei')

        # ç§»é™¤æ™‚å€ä¿¡æ¯ï¼Œçµ±ä¸€ç‚ºæœ¬åœ°æ™‚é–“
        hist_data.index = hist_data.index.tz_localize(None)

        # ç¢ºä¿æ•¸æ“šæŒ‰æ™‚é–“æ’åº
        hist_data = hist_data.sort_index()

        # ç²å–æ•¸æ“šçš„æ™‚é–“ç¯„åœ
        start_date = hist_data.index.min()
        end_date = hist_data.index.max()

        logger.info(f"ğŸ“Š æ•¸æ“šæ™‚é–“ç¯„åœ: {start_date.strftime('%Y-%m-%d')} åˆ° {end_date.strftime('%Y-%m-%d')}")

        if len(hist_data) < 90:
            logger.warning("âš ï¸ æ•¸æ“šä¸è¶³90å¤©ï¼Œç„¡æ³•è¨ˆç®—è½‰æŠ˜é»")
            return []

        points = []

        # ä¿®æ­£ï¼šä½¿ç”¨æ›´ç²¾ç¢ºçš„æœˆä»½è¨ˆç®—æ–¹æ³•
        # å°‡æ•¸æ“šæŒ‰æœˆä»½åˆ†çµ„
        hist_data['year_month'] = hist_data.index.to_period('M')
        monthly_groups = hist_data.groupby('year_month')

        # ç²å–æ‰€æœ‰æœˆä»½
        all_months = sorted(monthly_groups.groups.keys())

        logger.info(f"ğŸ“Š å¯ç”¨æœˆä»½: {[str(m) for m in all_months]}")

        # å¾ç¬¬4å€‹æœˆé–‹å§‹è¨ˆç®—ï¼ˆéœ€è¦å‰3å€‹æœˆçš„æ•¸æ“šï¼‰
        for i in range(3, len(all_months)):
            current_month = all_months[i]

            # ç²å–å‰ä¸‰å€‹æœˆçš„æ•¸æ“š
            prev3_months = all_months[i - 3:i]
            prev3_data = hist_data[hist_data['year_month'].isin(prev3_months)]

            if len(prev3_data) == 0:
                logger.warning(f"âš ï¸ æœˆä»½ {current_month} çš„å‰ä¸‰å€‹æœˆæ•¸æ“šä¸è¶³")
                continue

            # è¨ˆç®—å‰ä¸‰å€‹æœˆçš„æœ€é«˜åƒ¹å’Œæœ€ä½åƒ¹
            high = prev3_data['High'].max()
            low = prev3_data['Low'].min()
            pivot = (high + low) / 2

            # ç²å–ç•¶å‰æœˆä»½çš„æ•¸æ“š
            current_month_data = hist_data[hist_data['year_month'] == current_month]

            # ä¿®æ­£ï¼šç¢ºä¿æ¯å€‹æœˆéƒ½æœ‰ä¸€å€‹è½‰æŠ˜é»ï¼Œä½¿ç”¨ç•¶æœˆç¬¬ä¸€å€‹äº¤æ˜“æ—¥
            if len(current_month_data) > 0:
                # ä½¿ç”¨ç•¶æœˆç¬¬ä¸€å€‹äº¤æ˜“æ—¥
                first_trading_day = current_month_data.index.min()
                point_date = first_trading_day.strftime('%Y-%m-%d')
                logger.info(f"ğŸ“Š è½‰æŠ˜é»: {point_date} (æœˆä»½: {current_month}) = ${pivot:.2f}")
            else:
                # å¦‚æœç•¶æœˆæ²’æœ‰äº¤æ˜“æ•¸æ“šï¼Œä½¿ç”¨æœˆåˆæ—¥æœŸ
                point_date = current_month.to_timestamp().strftime('%Y-%m-%d')
                logger.warning(f"âš ï¸ ç•¶æœˆç„¡äº¤æ˜“æ•¸æ“šï¼Œä½¿ç”¨æœˆåˆ: {point_date}")

            points.append({
                'time': point_date,
                'price': float(pivot),
                'high': float(high),
                'low': float(low),
                'range': f"{prev3_months[0]}~{prev3_months[-1]}"
            })

        # æŒ‰æ™‚é–“æ’åºç¢ºä¿æŠ˜ç·šåœ–æ­£ç¢ºé€£æ¥
        points.sort(key=lambda x: x['time'])

        logger.info(f"ğŸ“Š è½‰æŠ˜é»è¨ˆç®—å®Œæˆï¼Œå…± {len(points)} å€‹æ•¸æ“šé»")
        if points:
            prices = [p['price'] for p in points]
            logger.info(f"    è½‰æŠ˜é»åƒ¹æ ¼ç¯„åœ: ${min(prices):.2f} - ${max(prices):.2f}")
            logger.info(f"    è½‰æŠ˜é»æ™‚é–“ç¯„åœ: {points[0]['time']} åˆ° {points[-1]['time']}")

            # æª¢æŸ¥æ¯å€‹æœˆçš„é»æ•¸
            monthly_counts = {}
            for point in points:
                month = point['time'][:7]  # å–YYYY-MMéƒ¨åˆ†
                monthly_counts[month] = monthly_counts.get(month, 0) + 1

            logger.info(f"    æ¯æœˆé»æ•¸çµ±è¨ˆ:")
            for month, count in sorted(monthly_counts.items()):
                logger.info(f"      {month}: {count} å€‹é»")

            # æª¢æŸ¥è½‰æŠ˜é»çš„é€£çºŒæ€§
            logger.info(f"    è½‰æŠ˜é»è©³ç´°ä¿¡æ¯:")
            for i, point in enumerate(points):
                logger.info(f"      {i + 1}. {point['time']} - ${point['price']:.2f} (åŸºæ–¼{point['range']})")
        else:
            logger.info("    ç„¡è½‰æŠ˜é»æ•¸æ“š")

        return points

    except Exception as e:
        logger.warning(f"âš ï¸ è½‰æŠ˜é»è¨ˆç®—éŒ¯èª¤: {e}")
        return []


def calculate_ma125_line(hist_data):
    """è¨ˆç®—MA125ç§»å‹•å¹³å‡ç·š"""
    try:
        # ç¢ºä¿æ•¸æ“šæœ‰æ—¥æœŸç´¢å¼•
        if not isinstance(hist_data.index, pd.DatetimeIndex):
            hist_data.index = pd.to_datetime(hist_data.index)

        if len(hist_data) < 125:
            logger.warning("âš ï¸ æ•¸æ“šä¸è¶³125å¤©ï¼Œç„¡æ³•è¨ˆç®—MA125")
            return []

        # è¨ˆç®—MA125
        ma_125_data = hist_data['Close'].rolling(window=125).mean().dropna()

        # è½‰æ›ç‚ºåœ–è¡¨æ•¸æ“šæ ¼å¼ï¼Œç¢ºä¿æ™‚é–“æ ¼å¼èˆ‡åœ–è¡¨æ•¸æ“šä¸€è‡´
        ma_125_line_data = []
        for idx, val in ma_125_data.items():
            # ç¢ºä¿æ™‚é–“æ ¼å¼èˆ‡åœ–è¡¨æ•¸æ“šä¸€è‡´ï¼Œä¸¦æ­£ç¢ºè™•ç†æ™‚å€
            if hasattr(idx, 'tz_localize'):
                if idx.tz is None:
                    # å‡è¨­æ˜¯UTCæ™‚é–“ï¼Œè½‰æ›ç‚ºå°åŒ—æ™‚é–“
                    idx_local = idx + timedelta(hours=8)
                else:
                    # è½‰æ›ç‚ºå°åŒ—æ™‚é–“
                    idx_local = idx.tz_convert('Asia/Taipei')
            else:
                # å‡è¨­æ˜¯UTCæ™‚é–“ï¼Œè½‰æ›ç‚ºå°åŒ—æ™‚é–“
                idx_local = idx + timedelta(hours=8)

            ma_125_line_data.append({
                'time': idx_local.strftime('%Y-%m-%d'),
                'price': float(val)
            })

        logger.info(f"ğŸ“Š MA125è¨ˆç®—å®Œæˆï¼Œå…± {len(ma_125_line_data)} å€‹æ•¸æ“šé»")
        logger.info(f"    æœ€æ–°MA125å€¼: ${ma_125_data.iloc[-1]:.2f}")

        return ma_125_line_data

    except Exception as e:
        logger.warning(f"âš ï¸ MA125è¨ˆç®—éŒ¯èª¤: {e}")
        return []


def detect_golden_death_cross(hist_data):
    """æª¢æ¸¬é»ƒé‡‘äº¤å‰å’Œæ­»äº¡äº¤å‰ - ä½¿ç”¨MA20ç©¿è¶ŠMA5"""
    try:
        if len(hist_data) < 20:
            return {"golden_cross": False, "death_cross": False, "message": "", "status": "normal"}

        # è¨ˆç®—MA20å’ŒMA5
        ma_20 = hist_data['Close'].rolling(window=20).mean()
        ma_5 = hist_data['Close'].rolling(window=5).mean()

        # ç²å–æœ€è¿‘å¹¾å€‹æ•¸æ“šé»é€²è¡Œæ¯”è¼ƒ
        recent_data = hist_data.tail(10)
        recent_ma20 = ma_20.tail(10)
        recent_ma5 = ma_5.tail(10)

        # æª¢æŸ¥æ˜¯å¦æœ‰è¶³å¤ çš„æ•¸æ“š
        if recent_ma20.isna().all() or recent_ma5.isna().all():
            return {"golden_cross": False, "death_cross": False, "message": "", "status": "normal"}

        # ç²å–æœ€æ–°çš„MAå€¼ï¼Œç¢ºä¿è½‰æ›ç‚ºPythonåŸç”Ÿé¡å‹
        current_ma20 = float(recent_ma20.iloc[-1])
        current_ma5 = float(recent_ma5.iloc[-1])
        prev_ma20 = float(recent_ma20.iloc[-2]) if len(recent_ma20) > 1 else current_ma20
        prev_ma5 = float(recent_ma5.iloc[-2]) if len(recent_ma5) > 1 else current_ma5

        # æª¢æ¸¬é»ƒé‡‘äº¤å‰ï¼ˆMA20å¾ä¸‹æ–¹ç©¿è¶ŠMA5ï¼‰
        golden_cross = bool((prev_ma20 < prev_ma5) and (current_ma20 > current_ma5))

        # æª¢æ¸¬æ­»äº¡äº¤å‰ï¼ˆMA20å¾ä¸Šæ–¹ç©¿è¶ŠMA5ï¼‰
        death_cross = bool((prev_ma20 > prev_ma5) and (current_ma20 < current_ma5))

        message = ""
        status = "normal"
        if golden_cross:
            message = "ğŸŸ¢ é»ƒé‡‘äº¤å‰ï¼šMA20ç©¿è¶ŠMA5å‘ä¸Šï¼Œçœ‹æ¼²ä¿¡è™Ÿ"
            status = "golden_cross"
            logger.info(f"ğŸŸ¢ æª¢æ¸¬åˆ°é»ƒé‡‘äº¤å‰: MA20=${current_ma20:.2f}, MA5=${current_ma5:.2f}")
        elif death_cross:
            message = "ğŸ”´ æ­»äº¡äº¤å‰ï¼šMA20ç©¿è¶ŠMA5å‘ä¸‹ï¼Œçœ‹è·Œä¿¡è™Ÿ"
            status = "death_cross"
            logger.info(f"ğŸ”´ æª¢æ¸¬åˆ°æ­»äº¡äº¤å‰: MA20=${current_ma20:.2f}, MA5=${current_ma5:.2f}")
        else:
            message = "âšª æ­£å¸¸ï¼šMA20èˆ‡MA5ç„¡äº¤å‰ä¿¡è™Ÿ"
            status = "normal"
            logger.info(f"âšª ç„¡äº¤å‰ä¿¡è™Ÿ: MA20=${current_ma20:.2f}, MA5=${current_ma5:.2f}")

        return {
            "golden_cross": golden_cross,
            "death_cross": death_cross,
            "status": status,
            "message": message,
            "current_ma20": current_ma20,
            "current_ma5": current_ma5
        }

    except Exception as e:
        logger.warning(f"âš ï¸ äº¤å‰æª¢æ¸¬éŒ¯èª¤: {e}")
        return {"golden_cross": False, "death_cross": False, "message": "", "status": "normal"}


def calculate_yearly_average_line(hist_data):
    """è¨ˆç®—å¹´å¹³å‡åƒ¹æ ¼ç·š - ä¿®æ­£ç‚ºçœŸæ­£çš„æ°´å¹³ç·š"""
    try:
        # ç¢ºä¿æ•¸æ“šæœ‰æ—¥æœŸç´¢å¼•
        if not isinstance(hist_data.index, pd.DatetimeIndex):
            hist_data.index = pd.to_datetime(hist_data.index)

        # è¨ˆç®—éå»ä¸€å¹´çš„å¹³å‡åƒ¹æ ¼
        one_year_ago = hist_data.index.max() - pd.DateOffset(years=1)
        yearly_data = hist_data[hist_data.index >= one_year_ago]

        if len(yearly_data) == 0:
            logger.warning("âš ï¸ æ²’æœ‰è¶³å¤ çš„æ•¸æ“šè¨ˆç®—å¹´å¹³å‡åƒ¹æ ¼")
            return []

        # è¨ˆç®—å¹´å¹³å‡åƒ¹æ ¼
        yearly_avg_price = yearly_data['Close'].mean()

        # å‰µå»ºä¸€æ¢æ°´å¹³ç·šï¼Œè¦†è“‹æ•´å€‹æ™‚é–“ç¯„åœ
        yearly_line_data = []
        for date in hist_data.index:
            yearly_line_data.append({
                'time': str(date)[:10],  # å–å‰10å€‹å­—ç¬¦ä½œç‚ºæ—¥æœŸ
                'price': float(yearly_avg_price)
            })

        logger.info(f"ğŸ“Š å¹´å¹³å‡åƒ¹æ ¼è¨ˆç®—å®Œæˆ: ${yearly_avg_price:.2f}")
        logger.info(f"    æ•¸æ“šç¯„åœ: {str(yearly_data.index.min())[:10]} è‡³ {str(yearly_data.index.max())[:10]}")
        logger.info(f"    æ•¸æ“šé»æ•¸: {len(yearly_data)}")

        return yearly_line_data

    except Exception as e:
        logger.warning(f"âš ï¸ å¹´å¹³å‡åƒ¹æ ¼è¨ˆç®—éŒ¯èª¤: {e}")
        return []


def determine_market_status():
    """åˆ¤æ–·å¸‚å ´ç‹€æ…‹ - é»ƒé‡‘æœŸè²¨å¸‚å ´æ™‚é–“ (ç¾æ±æ™‚é–“)"""
    try:
        from datetime import timezone, timedelta

        # ç²å–ç¾æ±æ™‚é–“
        # æ³¨æ„ï¼šé€™è£¡ç°¡åŒ–è™•ç†ï¼Œå¯¦éš›æ‡‰è©²è€ƒæ…®å¤ä»¤æ™‚é–“
        # ç¾æ±æ™‚é–“ = UTC - 5å°æ™‚ (æ¨™æº–æ™‚é–“) æˆ– UTC - 4å°æ™‚ (å¤ä»¤æ™‚é–“)
        utc_now = datetime.now(timezone.utc)

        # ç°¡åŒ–ï¼šå‡è¨­æ˜¯æ¨™æº–æ™‚é–“ (UTC - 5)
        # å¯¦éš›æ‡‰ç”¨ä¸­æ‡‰è©²ä½¿ç”¨ pytz æˆ– zoneinfo ä¾†æ­£ç¢ºè™•ç†æ™‚å€
        est_offset = timedelta(hours=5)
        est_now = utc_now - est_offset

        weekday = est_now.weekday()  # 0=Monday, 6=Sunday
        hour = est_now.hour
        minute = est_now.minute

        # é»ƒé‡‘æœŸè²¨å¸‚å ´æ™‚é–“ (ç¾æ±æ™‚é–“ EST)
        # é€±æ—¥ 6:00 PM - é€±äº” 5:00 PM (ç¾æ±æ™‚é–“)
        # é€±äº” 5:00 PM - é€±æ—¥ 6:00 PM ä¼‘å¸‚

        # èª¿è©¦ä¿¡æ¯
        logger.info(f"ğŸ” å¸‚å ´ç‹€æ…‹åˆ¤æ–·: UTC={utc_now.strftime('%Y-%m-%d %H:%M')}, "
                    f"EST={est_now.strftime('%Y-%m-%d %H:%M')}, "
                    f"é€±{weekday + 1}, {hour:02d}:{minute:02d}")

        if weekday < 5:  # Monday to Friday
            logger.info("âœ… é€±ä¸€åˆ°é€±äº” - é–‹å¸‚")
            return "open"  # é€±ä¸€åˆ°é€±äº”éƒ½æ˜¯é–‹å¸‚
        elif weekday == 5:  # Saturday
            logger.info("âŒ é€±å…­ - ä¼‘å¸‚")
            return "closed"  # é€±å…­ä¼‘å¸‚
        elif weekday == 6:  # Sunday
            # é€±æ—¥ 6:00 PM (18:00) å¾Œé–‹å¸‚
            if hour >= 18:
                logger.info("âœ… é€±æ—¥ 18:00å¾Œ - é–‹å¸‚")
                return "open"
            else:
                logger.info("âŒ é€±æ—¥ 18:00å‰ - ä¼‘å¸‚")
                return "closed"
        else:
            logger.info("âŒ æœªçŸ¥é€±æœŸ - ä¼‘å¸‚")
            return "closed"

    except Exception as e:
        logger.error(f"âŒ å¸‚å ´ç‹€æ…‹åˆ¤æ–·å¤±æ•—: {e}")
        return "unknown"


def get_market_name(info):
    """ç²å–å¸‚å ´åç¨±"""
    try:
        if isinstance(info, dict) and info:
            return info.get('longName', 'Gold Futures (GC=F)')
        return 'Gold Futures (GC=F)'
    except Exception as e:
        logger.error(f"âŒ ç²å–å¸‚å ´åç¨±å¤±æ•—: {e}")
        return 'Gold Futures (GC=F)'


def create_mock_gold_data(period: str):
    """å‰µå»ºæ¨¡æ“¬é»ƒé‡‘åƒ¹æ ¼æ•¸æ“šä½œç‚ºå‚™é¸æ–¹æ¡ˆ"""
    logger.info("ğŸ”§ ä½¿ç”¨æ¨¡æ“¬æ•¸æ“šä½œç‚ºå‚™é¸æ–¹æ¡ˆ")

    base_price = 2025.50
    current_price = base_price + np.random.uniform(-10, 10)
    change = np.random.uniform(-20, 20)
    change_percent = (change / base_price) * 100

    # æ ¹æ“šæ™‚é–“æœŸé–“ç”Ÿæˆæ¨¡æ“¬åœ–è¡¨æ•¸æ“š
    period_days = {
        '1d': 1, '5d': 5, '1mo': 30, '3mo': 90,
        '6mo': 180, '1y': 365
    }

    days = period_days.get(period, 365)
    chart_data = []

    for i in range(min(days, 100)):  # æœ€å¤š100å€‹æ•¸æ“šé»
        date = datetime.now() - timedelta(days=days - i - 1)
        price_variation = np.random.uniform(-0.02, 0.02)  # 2%çš„æ³¢å‹•
        price = base_price * (1 + price_variation)

        chart_data.append({
            "time": date.isoformat(),
            "price": round(price, 2),
            "high": round(price * 1.005, 2),
            "low": round(price * 0.995, 2),
            "open": round(price * (1 + np.random.uniform(-0.001, 0.001)), 2),
            "volume": np.random.randint(1000, 10000)
        })

    logger.info(f"ğŸ”§ ç”Ÿæˆæ¨¡æ“¬æ•¸æ“š: {len(chart_data)} å€‹æ•¸æ“šé»")

    return {
        "status": "success",
        "data": {
            "symbol": "GC=F",
            "name": "Gold Futures (æ¨¡æ“¬æ•¸æ“š)",
            "current_price": round(current_price, 2),
            "change": round(change, 2),
            "change_percent": round(change_percent, 2),
            "high_24h": round(current_price * 1.015, 2),
            "low_24h": round(current_price * 0.985, 2),
            "today_high": round(current_price * 1.008, 2),
            "today_low": round(current_price * 0.992, 2),
            "avg_price": round(base_price, 2),
            "volatility": round(np.random.uniform(10, 50), 2),
            "volume_24h": np.random.randint(50000, 200000),
            "currency": "USD",
            "unit": "per ounce",
            "last_updated": datetime.now().isoformat(),
            "chart_data": chart_data,
            "market_status": "open",
            "technical_indicators": {
                "ma_20": round(current_price * 0.998, 2),
                "ma_50": round(current_price * 1.002, 2),
                "rsi": round(np.random.uniform(30, 70), 1)
            },
            "period": period,
            "interval": "1d",
            "data_points": len(chart_data),
            "trading_days": len(chart_data)
        },
        "system_time": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
        "next_update": (datetime.now() + timedelta(minutes=5)).strftime("%Y-%m-%d %H:%M:%S"),
        "data_source": "Mock Data (Yahoo Finance ä¸å¯ç”¨)"
    }


@app.post("/api/send-mail-to-n8n")
async def send_mail_to_n8n(mail_data: MailSenderRequest):
    """ç™¼é€éƒµä»¶æ•¸æ“šåˆ° N8N webhook"""
    try:
        # logger.info(f"ğŸ“§ æ”¶åˆ°éƒµä»¶ç™¼é€è«‹æ±‚:")
        # logger.info(f"   æ”¶ä»¶äºº: {mail_data.recipient_email}")
        # logger.info(f"   è‡ªè¨‚è¨Šæ¯: {mail_data.custom_message[:50] if mail_data.custom_message else 'ç„¡'}...")
        # logger.info(f"   ä¸»é¡Œ: {mail_data.subject}")

        if not stored_data:
            logger.error("âŒ æ²’æœ‰å¯ç”¨çš„å¸‚å ´åˆ†æè³‡æ–™")
            raise HTTPException(status_code=400, detail="æ²’æœ‰å¯ç”¨çš„å¸‚å ´åˆ†æè³‡æ–™")

        # logger.info(f"ğŸ“Š ç•¶å‰å„²å­˜æ•¸æ“š: {len(stored_data)} å€‹æ¬„ä½")
        # logger.info(f"   æƒ…æ„Ÿåˆ†æ•¸: {stored_data.get('average_sentiment_score', 'N/A')}")
        # logger.info(f"   å…§å®¹é•·åº¦: {len(stored_data.get('message_content', ''))} å­—å…ƒ")

        # æ§‹å»ºç™¼é€åˆ° N8N çš„æ•¸æ“šçµæ§‹
        send_data = {
            **stored_data,
            "mail_config": {
                "recipient_email": str(mail_data.recipient_email),
                "sender_name": mail_data.sender_name or "å¸‚å ´åˆ†æç³»çµ±",
                "subject": mail_data.subject or "å¸‚å ´åˆ†æå ±å‘Š",
                "priority": mail_data.priority or "normal",
                "mail_type": mail_data.mail_type or "daily",
                "custom_message": mail_data.custom_message or "",
                "include_charts": mail_data.include_charts,
                "include_recommendations": mail_data.include_recommendations,
                "include_risk_warning": mail_data.include_risk_warning
            },
            "system_info": {
                "send_timestamp": datetime.now().isoformat(),
                "system_version": CONFIG['SYSTEM_INFO']['version'],
                "source": "mail-sender-page"
            },
            "sentiment_analysis": {
                "score": stored_data.get("average_sentiment_score", 0),
                "text": get_sentiment_text(stored_data.get("average_sentiment_score", 0)),
                "emoji": get_market_emoji(stored_data.get("average_sentiment_score", 0))
            }
        }

        # logger.info(f"ğŸ“¤ æº–å‚™ç™¼é€æ•¸æ“šåˆ° N8N:")
        # logger.info(f"   Webhook URL: {CONFIG['WEBHOOK_CONFIG']['n8n_webhook_url']}")
        # logger.info(f"   æ•¸æ“šå¤§å°: {len(json.dumps(send_data, ensure_ascii=False))} å­—å…ƒ")
        #
        # # è¼¸å‡ºå®Œæ•´çš„ JSON æ•¸æ“š
        # logger.info("ğŸ“‹ ç™¼é€åˆ° N8N çš„å®Œæ•´ JSON æ•¸æ“š:")
        # logger.info(json.dumps(send_data, ensure_ascii=False, indent=2))
        #
        # logger.info("ğŸ“¤ é–‹å§‹ç™¼é€æ•¸æ“šåˆ° N8N...")

        response = requests.post(
            CONFIG['WEBHOOK_CONFIG']['n8n_webhook_url'],
            json=send_data,
            headers={'Content-Type': 'application/json'},
            timeout=CONFIG['WEBHOOK_CONFIG']['timeout']
        )

        # logger.info(f"ğŸ“¡ N8N å›æ‡‰ç‹€æ…‹: {response.status_code}")
        # logger.info(f"ğŸ“¡ N8N å›æ‡‰å…§å®¹: {response.text[:200]}...")

        if response.status_code == 200:
            # logger.info("âœ… éƒµä»¶æ•¸æ“šå·²æˆåŠŸç™¼é€åˆ° N8N")
            return {
                "status": "success",
                "message": f"éƒµä»¶æ•¸æ“šå·²æˆåŠŸç™¼é€åˆ° N8N",
                "sent_time": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                "recipient": str(mail_data.recipient_email),
                "n8n_response": response.text[:100] if response.text else "ç„¡å›æ‡‰å…§å®¹"
            }
        else:
            logger.error(f"âŒ N8N webhook å›æ‡‰éŒ¯èª¤: {response.status_code} - {response.text}")
            raise HTTPException(status_code=response.status_code, detail=f"N8N webhook å›æ‡‰éŒ¯èª¤: {response.text}")

    except requests.exceptions.Timeout:
        logger.error("âŒ è«‹æ±‚è¶…æ™‚")
        raise HTTPException(status_code=500, detail="è«‹æ±‚è¶…æ™‚")
    except requests.exceptions.ConnectionError:
        logger.error("âŒ ç„¡æ³•é€£æ¥åˆ° N8N webhook")
        raise HTTPException(status_code=500, detail="ç„¡æ³•é€£æ¥åˆ° N8N webhook")
    except Exception as e:
        logger.error(f"âŒ ç™¼é€éƒµä»¶åˆ° N8N å¤±æ•—: {str(e)}")
        raise HTTPException(status_code=500, detail=f"ç™¼é€å¤±æ•—: {str(e)}")


@app.get("/api/test-n8n-connection")
async def test_n8n_connection():
    """æ¸¬è©¦ N8N é€£æ¥"""
    try:
        response = requests.get(
            CONFIG['WEBHOOK_CONFIG']['n8n_webhook_url'],
            timeout=10
        )
        return {
            "status": "success",
            "message": "N8N é€£æ¥æ­£å¸¸",
            "status_code": response.status_code,
            "url": CONFIG['WEBHOOK_CONFIG']['n8n_webhook_url']
        }
    except Exception as e:
        return {
            "status": "error",
            "message": f"N8N é€£æ¥å¤±æ•—: {str(e)}",
            "url": CONFIG['WEBHOOK_CONFIG']['n8n_webhook_url']
        }


@app.get("/api/debug-stored-data")
async def debug_stored_data():
    """èª¿è©¦ç«¯é» - æŸ¥çœ‹ç•¶å‰å­˜å„²çš„æ•¸æ“šçµæ§‹"""
    try:
        if not stored_data:
            return {
                "status": "warning",
                "message": "æ²’æœ‰å­˜å„²çš„æ•¸æ“š",
                "data": None,
                "timestamp": datetime.now().isoformat()
            }

        # æ§‹å»ºç¤ºä¾‹éƒµä»¶æ•¸æ“šï¼ˆä¸å¯¦éš›ç™¼é€ï¼‰
        sample_mail_data = {
            "recipient_email": "test@example.com",
            "custom_message": "é€™æ˜¯ä¸€å€‹æ¸¬è©¦è¨Šæ¯"
        }

        # æ¨¡æ“¬éƒµä»¶ç™¼é€æ™‚çš„æ•¸æ“šçµæ§‹
        sample_send_data = {
            **stored_data,
            "mail_config": {
                "recipient_email": sample_mail_data["recipient_email"],
                "sender_name": "å¸‚å ´åˆ†æç³»çµ±",
                "subject": "å¸‚å ´åˆ†æå ±å‘Š",
                "priority": "normal",
                "mail_type": "daily",
                "custom_message": sample_mail_data["custom_message"],
                "include_charts": False,
                "include_recommendations": False,
                "include_risk_warning": False
            },
            "system_info": {
                "send_timestamp": datetime.now().isoformat(),
                "system_version": CONFIG['SYSTEM_INFO']['version'],
                "source": "debug-endpoint"
            },
            "sentiment_analysis": {
                "score": stored_data.get("average_sentiment_score", 0),
                "text": get_sentiment_text(stored_data.get("average_sentiment_score", 0)),
                "emoji": get_market_emoji(stored_data.get("average_sentiment_score", 0))
            }
        }

        return {
            "status": "success",
            "message": "ç•¶å‰å­˜å„²çš„æ•¸æ“šçµæ§‹",
            "json_data": sample_send_data,
            "timestamp": datetime.now().isoformat(),
            "webhook_url": CONFIG['WEBHOOK_CONFIG']['n8n_webhook_url']
        }

    except Exception as e:
        logger.error(f"âŒ èª¿è©¦æ•¸æ“šç«¯é»éŒ¯èª¤: {str(e)}")
        return {
            "status": "error",
            "message": f"èª¿è©¦æ•¸æ“šå¤±æ•—: {str(e)}",
            "timestamp": datetime.now().isoformat()
        }


@app.get("/health")
async def health_check():
    """ç³»çµ±å¥åº·æª¢æŸ¥ - å¢å¼·ç‰ˆæœ¬"""
    uptime = datetime.now() - system_stats["uptime_start"]

    # æ¸¬è©¦é»ƒé‡‘åƒ¹æ ¼ API
    gold_api_status = "healthy"
    try:
        import yfinance as yf
        test_ticker = yf.Ticker("GC=F")
        test_data = test_ticker.history(period="1d", interval="1d")
        if test_data.empty:
            gold_api_status = "degraded"
    except Exception:
        gold_api_status = "unhealthy"

    return {
        "status": "healthy",
        "timestamp": datetime.now().isoformat(),
        "system": CONFIG['SYSTEM_INFO']['name'],
        "version": CONFIG['SYSTEM_INFO']['version'],
        "has_market_data": len(stored_data) > 0,
        "uptime": str(uptime).split('.')[0],
        "environment": os.getenv('ENVIRONMENT', 'development'),
        "stats": system_stats,
        "features": {
            "gold_price_api": gold_api_status,
            "market_analysis": "healthy",
            "mail_sender": "healthy",
            "real_time_updates": "healthy"
        },
        "services": {
            "yfinance": gold_api_status,
            "n8n_webhook": "unknown",
            "static_files": "healthy"
        }
    }


# è¼”åŠ©å‡½æ•¸
def get_sentiment_text(score: float) -> str:
    """æ ¹æ“šæƒ…æ„Ÿåˆ†æ•¸è¿”å›æ–‡å­—æè¿°"""
    if score > 0.6:
        return "æ¥µåº¦æ¨‚è§€"
    elif score > 0.2:
        return "æ¨‚è§€"
    elif score > 0.1:
        return "ä¸­æ€§åæ¨‚è§€"
    elif score > -0.1:
        return "ä¸­æ€§"
    elif score > -0.2:
        return "ä¸­æ€§åæ‚²è§€"
    elif score > -0.6:
        return "æ‚²è§€"
    else:
        return "æ¥µåº¦æ‚²è§€"


def get_market_emoji(score: float) -> str:
    """æ ¹æ“šæƒ…æ„Ÿåˆ†æ•¸è¿”å›è¡¨æƒ…ç¬¦è™Ÿ"""
    if score > 0.6:
        return "ğŸš€ğŸ“ˆğŸ’š"
    elif score > 0.2:
        return "ğŸ“ˆğŸŸ¢ğŸ˜Š"
    elif score > 0.1:
        return "ğŸ“ŠğŸŸ¡ğŸ˜"
    elif score > -0.1:
        return "â¡ï¸âšªğŸ˜‘"
    elif score > -0.2:
        return "ğŸ“ŠğŸŸ¡ğŸ˜"
    elif score > -0.6:
        return "ğŸ“‰ğŸ”´ğŸ˜Ÿ"
    else:
        return "ğŸ’¥ğŸ“‰ğŸ˜±"


# éŒ¯èª¤è™•ç†
@app.exception_handler(HTTPException)
async def http_exception_handler(request: Request, exc: HTTPException):
    logger.error(f"HTTPç•°å¸¸: {exc.status_code} - {exc.detail}")
    return JSONResponse(
        status_code=exc.status_code,
        content={
            "status": "error",
            "message": exc.detail,
            "timestamp": datetime.now().isoformat(),
            "path": str(request.url)
        }
    )


@app.exception_handler(Exception)
async def general_exception_handler(request: Request, exc: Exception):
    logger.error(f"æœªè™•ç†çš„ç•°å¸¸: {str(exc)}")
    system_stats["errors"] += 1
    return JSONResponse(
        status_code=500,
        content={
            "status": "error",
            "message": "å…§éƒ¨æœå‹™å™¨éŒ¯èª¤",
            "timestamp": datetime.now().isoformat(),
            "path": str(request.url)
        }
    )


def main():
    uvicorn.run(
        app,
        host=CONFIG['SERVER_CONFIG']['host'],
        port=CONFIG['SERVER_CONFIG']['port'],
        log_level="info",
        reload=CONFIG['SERVER_CONFIG'].get('debug', False)
    )


if __name__ == "__main__":
    main()